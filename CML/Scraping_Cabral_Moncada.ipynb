{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_rows', 100)  \n",
    "\n",
    "#Beautiful Soup and Requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "#Import Regex\n",
    "import re\n",
    "\n",
    "#URLIB\n",
    "import urllib\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final page reached!\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cml.pt/leiloes/online'\n",
    "dict_url_links = {}\n",
    "flag = True\n",
    "while flag:\n",
    "    \n",
    "    s = requests.Session()\n",
    "    soup = BeautifulSoup(requests_retry_session(session=s).get(url).content)\n",
    "\n",
    "    #Get all urls for modern and contemporary art auctions\n",
    "    list_url = soup.find_all('a', href=re.compile(\"leilao-online-de-arte-moderna-e-contemporanea\"))\n",
    "\n",
    "    #Save links on the page to a dictionary of \"auction number : link\" pairs\n",
    "    for i in list_url:\n",
    "        dict_url_links[int(i['href'].split('/')[-2])] = i['href']\n",
    "\n",
    "    #Get next page link\n",
    "    list_next_page = soup.find_all('a', href=True, text='Seguinte')\n",
    "    if len(list_next_page)>0:\n",
    "        url = list_next_page[0]['href']\n",
    "    else:\n",
    "        flag=False\n",
    "        print('Final page reached!')\n",
    "        \n",
    "#Store links in a DataFrame for saving\n",
    "df_auction_links = pd.DataFrame(data=dict_url_links.values(),index=dict_url_links.keys(),columns=['URL'])\n",
    "#Load previous auction_links\n",
    "df_auction_links_previous = pd.read_csv('cml_auction_links.csv',index_col=0)\n",
    "#Save auction links to drive\n",
    "df_auction_links.to_csv('cml_auction_links_new.csv')\n",
    "\n",
    "to_run= False\n",
    "#If we want to run analysis only on the new auctions, run the following if and get the dataframe of only the new auctions\n",
    "if to_run==True:\n",
    "    dfs_dictionary = {'DF1':df_auction_links,'DF2':df_auction_links_previous}\n",
    "    df = pd.concat(dfs_dictionary)\n",
    "    df = df.drop_duplicates(keep=False).copy()\n",
    "else:\n",
    "    df = df_auction_links.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final page of auction 1222 reached!\n",
      "458 artworks were scraped.\n",
      "Final page of auction 1218 reached!\n",
      "319 artworks were scraped.\n",
      "Final page of auction 1199 reached!\n",
      "301 artworks were scraped.\n",
      "Final page of auction 1183 reached!\n",
      "347 artworks were scraped.\n",
      "Final page of auction 1170 reached!\n",
      "301 artworks were scraped.\n",
      "Final page of auction 1142 reached!\n",
      "319 artworks were scraped.\n",
      "Final page of auction 1120 reached!\n",
      "301 artworks were scraped.\n",
      "Final page of auction 1098 reached!\n",
      "250 artworks were scraped.\n",
      "Final page of auction 1084 reached!\n",
      "280 artworks were scraped.\n",
      "Final page of auction 1066 reached!\n",
      "257 artworks were scraped.\n",
      "Final page of auction 1047 reached!\n",
      "287 artworks were scraped.\n",
      "Final page of auction 1034 reached!\n",
      "188 artworks were scraped.\n",
      "Final page of auction 1025 reached!\n",
      "212 artworks were scraped.\n",
      "Final page of auction 1014 reached!\n",
      "190 artworks were scraped.\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n"
     ]
    },
    {
     "ename": "UnicodeError",
     "evalue": "UTF-16 stream does not start with BOM",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5757767cd7fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;31m#Concatenate_the artworks on the existing DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m \u001b[0mdf_artworks_previous\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'artworks.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-16'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[0mdf_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_artworks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_artworks_previous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeError\u001b[0m: UTF-16 stream does not start with BOM"
     ]
    }
   ],
   "source": [
    "#Create a dictionary for the data\n",
    "dic_artworks = {'Auction Number':[],\n",
    "               'Artwork Number':[],\n",
    "               'Author':[],\n",
    "               'Title':[],\n",
    "               'Technique':[],\n",
    "               'Dimensions':[],\n",
    "               'Estimated Price':[],\n",
    "               'Date of Auction End':[],\n",
    "               'Sale Price':[],\n",
    "               'Image URL':[]}\n",
    "\n",
    "#Get list of auction URL's\n",
    "list_auction_url = df['URL'].tolist()\n",
    "\n",
    "for auction_nbr in range(len(list_auction_url)):\n",
    "    #Loop over the auction url's\n",
    "    url_auction = list_auction_url[auction_nbr]\n",
    "    url = url_auction\n",
    "    #Record the auction number\n",
    "    tmp_auction_nbr = df.index[auction_nbr]\n",
    "\n",
    "    #Loop over all pages of the auction and scrape the artworks\n",
    "    flag = True\n",
    "    counter=1\n",
    "    while flag == True:\n",
    "        s = requests.Session()\n",
    "        soup = BeautifulSoup(requests_retry_session(session=s).get(url).content)\n",
    "        tmp_list = soup.find_all('div',attrs={'class':'mdl-cell mdl-card mdl-shadow--4dp portfolio-card loteCard'})\n",
    "        for entry in range(len(tmp_list)):\n",
    "            #Loop over all the artworks on the page\n",
    "\n",
    "            dic_artworks.setdefault('Auction Number', []).append(tmp_auction_nbr)\n",
    "            dic_artworks.setdefault('Artwork Number', []).append(counter)\n",
    "\n",
    "            try:\n",
    "                tmp_author = tmp_list[entry].find('span',attrs={'class':'dsp_autor'}).text.rstrip('\\n')\n",
    "                dic_artworks.setdefault('Author', []).append(tmp_author)\n",
    "            except:\n",
    "                tmp_author = np.nan\n",
    "                dic_artworks.setdefault('Author', []).append(tmp_author)\n",
    "            try:\n",
    "                tmp_title = tmp_list[entry].find('span',attrs={'class':'dsp_designacao'}).text\n",
    "                dic_artworks.setdefault('Title', []).append(tmp_title)\n",
    "            except:\n",
    "                tmp_title = np.nan\n",
    "                dic_artworks.setdefault('Title', []).append(tmp_title)\n",
    "            try:   \n",
    "                tmp_technique = tmp_list[entry].find_all(\"span\", id=re.compile(r\"detalhesLotePt\"))[0].text\n",
    "                dic_artworks.setdefault('Technique', []).append(tmp_technique)\n",
    "            except:\n",
    "                tmp_technique = np.nan\n",
    "                dic_artworks.setdefault('Technique', []).append(tmp_technique)\n",
    "            try:\n",
    "                tmp_dim = [i for i in tmp_list[entry].text.split('\\n') if 'dim' in i.lower()][0]\n",
    "                dic_artworks.setdefault('Dimensions', []).append(tmp_dim)\n",
    "            except:\n",
    "                tmp_dim = np.nan   \n",
    "                dic_artworks.setdefault('Dimensions', []).append(tmp_dim)\n",
    "            try:\n",
    "                tmp_estimated_price = int(re.findall(r'\\d+',tmp_list[entry].find('p',attrs={'class':'lote-base-estimativa'}).text)[0])\n",
    "                dic_artworks.setdefault('Estimated Price', []).append(tmp_estimated_price)\n",
    "            except:\n",
    "                tmp_estimated_price = np.nan \n",
    "                dic_artworks.setdefault('Estimated Price', []).append(tmp_estimated_price)\n",
    "            try:    \n",
    "                tmp_date_auction = tmp_list[entry].find_all(\"p\", id=re.compile(r\"dataFim\"))[0].text.split('\\ndone')[1].split('Terminado')[0]\n",
    "                dic_artworks.setdefault('Date of Auction End', []).append(tmp_date_auction)\n",
    "            except:\n",
    "                tmp_date_auction = np.nan  \n",
    "                dic_artworks.setdefault('Date of Auction End', []).append(tmp_date_auction)\n",
    "            try:\n",
    "                tmp_sale_price = tmp_list[entry].find_all(\"p\", id=re.compile(r\"vendido\"))[0].text.split('€ ')[1].split('Vendido')[0]\n",
    "                dic_artworks.setdefault('Sale Price', []).append(tmp_sale_price)\n",
    "            except:\n",
    "            #    tmp_sale_price = int(tmp_list[entry].find_all(\"p\", id=re.compile(r\"retirado\"))[0].text.split('€ ')[1].split('Retirado')[0])\n",
    "                tmp_sale_price = 'Not sold'\n",
    "                dic_artworks.setdefault('Sale Price', []).append(tmp_sale_price)\n",
    "            try:    \n",
    "                tmp_img_url = tmp_list[entry].find_all('img')[0]['src']\n",
    "                dic_artworks.setdefault('Image URL', []).append(tmp_img_url)\n",
    "            except:\n",
    "                tmp_img_url = np.nan  \n",
    "                dic_artworks.setdefault('Image URL', []).append(tmp_img_url)\n",
    "            counter += 1\n",
    "\n",
    "        #Get next page link\n",
    "        list_next_page = soup.find_all('a', href=True, text='Seguinte')\n",
    "        if len(list_next_page)>0:\n",
    "            url = list_next_page[0]['href']\n",
    "        else:\n",
    "            flag=False\n",
    "            print('Final page of auction {} reached!\\n{} artworks were scraped.'.format(tmp_auction_nbr,counter))    \n",
    "            \n",
    "#Create DataFrame with the artworks\n",
    "df_artworks = pd.DataFrame(dic_artworks)\n",
    "\n",
    "#Collect Images\n",
    "for i in range(df_artworks.shape[0]):\n",
    "    #Download images\n",
    "    try:\n",
    "        urllib.request.urlretrieve(df_artworks.iloc[i]['Image URL'], 'artwork_images/' + str(df_artworks.iloc[i]['Auction Number']) + '_'+ str(df_artworks.iloc[i]['Artwork Number']) +'.jpg')\n",
    "    except:\n",
    "        print('Um erro foi encontrado')\n",
    "\n",
    "#Concatenate_the artworks on the existing DataFrame\n",
    "df_artworks_previous = pd.read_csv('artworks.csv',encoding='utf-16')\n",
    "df_final = pd.concat((df_artworks,df_artworks_previous),axis=0)\n",
    "\n",
    "#Save the final DataFrame\n",
    "df_final.to_csv('artworks.csv',encoding='utf-16')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
