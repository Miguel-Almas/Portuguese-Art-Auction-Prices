{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_rows', 100)  \n",
    "\n",
    "#Beautiful Soup and Requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "#Import Regex\n",
    "import re\n",
    "\n",
    "#URLIB\n",
    "import urllib\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final page reached!\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cml.pt/leiloes/online'\n",
    "dict_url_links = {}\n",
    "flag = True\n",
    "while flag:\n",
    "    \n",
    "    s = requests.Session()\n",
    "    soup = BeautifulSoup(requests_retry_session(session=s).get(url).content)\n",
    "\n",
    "    #Get all urls for modern and contemporary art auctions\n",
    "    #list_url = soup.find_all('a', href=re.compile(\"leilao-online-de-arte-moderna-e-contemporanea\"))\n",
    "    list_url = soup.find_all('a', href=re.compile(\"arte-moderna-e-contemporanea\"))\n",
    "\n",
    "    #Save links on the page to a dictionary of \"auction number : link\" pairs\n",
    "    for i in list_url:\n",
    "        dict_url_links[int(i['href'].split('/')[-2])] = i['href']\n",
    "\n",
    "    #Get next page link\n",
    "    list_next_page = soup.find_all('a', href=True, text='Seguinte')\n",
    "    if len(list_next_page)>0:\n",
    "        url = list_next_page[0]['href']\n",
    "    else:\n",
    "        flag=False\n",
    "        print('Final page reached!')\n",
    "        \n",
    "#Store links in a DataFrame for saving\n",
    "df_auction_links = pd.DataFrame(data=dict_url_links.values(),index=dict_url_links.keys(),columns=['URL'])\n",
    "#Load previous auction_links\n",
    "df_auction_links_previous = pd.read_csv('cml_auction_links.csv',index_col=0)\n",
    "#Save auction links to drive\n",
    "df_auction_links.to_csv('cml_auction_links_new.csv')\n",
    "\n",
    "to_run= True\n",
    "#If we want to run analysis only on the new auctions, run the following if and get the dataframe of only the new auctions\n",
    "if to_run==True:\n",
    "    dfs_dictionary = {'DF1':df_auction_links,'DF2':df_auction_links_previous}\n",
    "    df = pd.concat(dfs_dictionary)\n",
    "    df = df.drop_duplicates(keep=False).copy()\n",
    "    df = df.reset_index().drop('level_0',axis=1).set_index('level_1')\n",
    "else:\n",
    "    df = df_auction_links.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final page of auction 1267 reached!\n",
      "178 artworks were scraped.\n",
      "Final page of auction 1247 reached!\n",
      "271 artworks were scraped.\n",
      "Final page of auction 1239 reached!\n",
      "110 artworks were scraped.\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n",
      "Um erro foi encontrado\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary for the data\n",
    "dic_artworks = {'Auction Number':[],\n",
    "               'Artwork Number':[],\n",
    "               'Author':[],\n",
    "               'Title':[],\n",
    "               'Technique':[],\n",
    "               'Dimensions':[],\n",
    "               'Estimated Price':[],\n",
    "               'Date of Auction End':[],\n",
    "               'Sale Price':[],\n",
    "               'Image URL':[]}\n",
    "\n",
    "#Get list of auction URL's\n",
    "list_auction_url = df['URL'].tolist()\n",
    "\n",
    "for auction_nbr in range(len(list_auction_url)):\n",
    "    #Loop over the auction url's\n",
    "    url_auction = list_auction_url[auction_nbr]\n",
    "    url = url_auction\n",
    "    #Record the auction number\n",
    "    tmp_auction_nbr = df.index[auction_nbr]\n",
    "\n",
    "    #Loop over all pages of the auction and scrape the artworks\n",
    "    flag = True\n",
    "    counter=1\n",
    "    while flag == True:\n",
    "        s = requests.Session()\n",
    "        soup = BeautifulSoup(requests_retry_session(session=s).get(url).content)\n",
    "        tmp_list = soup.find_all('div',attrs={'class':'mdl-cell mdl-card mdl-shadow--4dp portfolio-card loteCard'})\n",
    "        for entry in range(len(tmp_list)):\n",
    "            #Loop over all the artworks on the page\n",
    "\n",
    "            dic_artworks.setdefault('Auction Number', []).append(tmp_auction_nbr)\n",
    "            dic_artworks.setdefault('Artwork Number', []).append(counter)\n",
    "\n",
    "            try:\n",
    "                tmp_author = tmp_list[entry].find('span',attrs={'class':'dsp_autor'}).text.rstrip('\\n')\n",
    "                dic_artworks.setdefault('Author', []).append(tmp_author)\n",
    "            except:\n",
    "                tmp_author = np.nan\n",
    "                dic_artworks.setdefault('Author', []).append(tmp_author)\n",
    "            try:\n",
    "                tmp_title = tmp_list[entry].find('span',attrs={'class':'dsp_designacao'}).text\n",
    "                dic_artworks.setdefault('Title', []).append(tmp_title)\n",
    "            except:\n",
    "                tmp_title = np.nan\n",
    "                dic_artworks.setdefault('Title', []).append(tmp_title)\n",
    "            try:   \n",
    "                tmp_technique = tmp_list[entry].find_all(\"span\", id=re.compile(r\"detalhesLotePt\"))[0].text\n",
    "                dic_artworks.setdefault('Technique', []).append(tmp_technique)\n",
    "            except:\n",
    "                tmp_technique = np.nan\n",
    "                dic_artworks.setdefault('Technique', []).append(tmp_technique)\n",
    "            try:\n",
    "                tmp_dim = [i for i in tmp_list[entry].text.split('\\n') if 'dim' in i.lower()][0]\n",
    "                dic_artworks.setdefault('Dimensions', []).append(tmp_dim)\n",
    "            except:\n",
    "                tmp_dim = np.nan   \n",
    "                dic_artworks.setdefault('Dimensions', []).append(tmp_dim)\n",
    "            try:\n",
    "                tmp_estimated_price = int(re.findall(r'\\d+',tmp_list[entry].find('p',attrs={'class':'lote-base-estimativa'}).text)[0])\n",
    "                dic_artworks.setdefault('Estimated Price', []).append(tmp_estimated_price)\n",
    "            except:\n",
    "                tmp_estimated_price = np.nan \n",
    "                dic_artworks.setdefault('Estimated Price', []).append(tmp_estimated_price)\n",
    "            try:    \n",
    "                tmp_date_auction = tmp_list[entry].find_all(\"p\", id=re.compile(r\"dataFim\"))[0].text.split('\\ndone')[1].split('Terminado')[0]\n",
    "                dic_artworks.setdefault('Date of Auction End', []).append(tmp_date_auction)\n",
    "            except:\n",
    "                tmp_date_auction = np.nan  \n",
    "                dic_artworks.setdefault('Date of Auction End', []).append(tmp_date_auction)\n",
    "            try:\n",
    "                tmp_sale_price = tmp_list[entry].find_all(\"p\", id=re.compile(r\"vendido\"))[0].text.split('€ ')[1].split('Vendido')[0]\n",
    "                dic_artworks.setdefault('Sale Price', []).append(tmp_sale_price)\n",
    "            except:\n",
    "            #    tmp_sale_price = int(tmp_list[entry].find_all(\"p\", id=re.compile(r\"retirado\"))[0].text.split('€ ')[1].split('Retirado')[0])\n",
    "                tmp_sale_price = 'Not sold'\n",
    "                dic_artworks.setdefault('Sale Price', []).append(tmp_sale_price)\n",
    "            try:    \n",
    "                tmp_img_url = tmp_list[entry].find_all('img')[0]['src']\n",
    "                dic_artworks.setdefault('Image URL', []).append(tmp_img_url)\n",
    "            except:\n",
    "                tmp_img_url = np.nan  \n",
    "                dic_artworks.setdefault('Image URL', []).append(tmp_img_url)\n",
    "            counter += 1\n",
    "\n",
    "        #Get next page link\n",
    "        list_next_page = soup.find_all('a', href=True, text='Seguinte')\n",
    "        if len(list_next_page)>0:\n",
    "            url = list_next_page[0]['href']\n",
    "        else:\n",
    "            flag=False\n",
    "            print('Final page of auction {} reached!\\n{} artworks were scraped.'.format(tmp_auction_nbr,counter))    \n",
    "            \n",
    "#Create DataFrame with the artworks\n",
    "df_artworks = pd.DataFrame(dic_artworks)\n",
    "\n",
    "#Collect Images\n",
    "for i in range(df_artworks.shape[0]):\n",
    "    #Download images\n",
    "    try:\n",
    "        urllib.request.urlretrieve(df_artworks.iloc[i]['Image URL'], 'artwork_images/' + str(df_artworks.iloc[i]['Auction Number']) + '_'+ str(df_artworks.iloc[i]['Artwork Number']) +'.jpg')\n",
    "    except:\n",
    "        print('Um erro foi encontrado')\n",
    "\n",
    "#Concatenate_the artworks on the existing DataFrame\n",
    "df_artworks_previous = pd.read_csv('artworks.csv',encoding='utf-16')\n",
    "df_final = pd.concat((df_artworks,df_artworks_previous),axis=0)\n",
    "\n",
    "#Save the final DataFrame\n",
    "df_final.to_csv('artworks.csv',encoding='utf-16')\n",
    "\n",
    "dfs_dictionary = {'DF1':df_auction_links,'DF2':df_auction_links_previous}\n",
    "df = pd.concat(dfs_dictionary)\n",
    "df = df.drop_duplicates(keep='first').copy()\n",
    "df = df.reset_index().drop('level_0',axis=1).rename(columns={'level_1':''}).set_index('')\n",
    "df.to_csv('cml_auction_links_new.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
